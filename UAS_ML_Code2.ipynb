{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UAS-ML_Code2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "142hyEf1UaWsGdqk1PahSdp7prQyyI87H",
      "authorship_tag": "ABX9TyPmaXyijF6A9YoCSkpEOvga",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lahmuddin/Machine-Learning-after-Middle-Test/blob/main/UAS_ML_Code2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lahmuddin_1103184028_TK-42-G6**"
      ],
      "metadata": {
        "id": "8A4FCpEjU4h8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Installation**"
      ],
      "metadata": {
        "id": "hD6v91c-Voar"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lROtOfjdUv69",
        "outputId": "22eb1335-84ce-4bc2-9548-8e7e34f00a2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras\n",
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "AV732Xk5bNe6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "import re\n",
        "import sys\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "import random\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from scipy.sparse import lil_matrix\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "import nltk\n",
        "import pickle\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from sklearn import svm\n",
        "\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense\n",
        "\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Embedding\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.layers import LSTM\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.layers import Embedding, Flatten\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from numpy import loadtxt\n",
        "import pickle\n",
        "\n",
        "import glob\n",
        "\n",
        "import csv"
      ],
      "metadata": {
        "id": "4UTKuD4UbVjK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "\n",
        "def file_to_wordset(filename):\n",
        "    ''' Converts a file with a word per line to a Python set '''\n",
        "    words = []\n",
        "    with open(filename, 'r') as f:\n",
        "        for line in f:\n",
        "            words.append(line.strip())\n",
        "    return set(words)\n",
        "\n",
        "\n",
        "def write_status(i, total):\n",
        "    ''' Writes status of a process to console '''\n",
        "    sys.stdout.write('\\r')\n",
        "    sys.stdout.write('Processing %d/%d' % (i, total))\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "def save_results_to_csv(results, csv_file):\n",
        "    ''' Save list of type [(tweet_id, positive)] to csv in Kaggle format '''\n",
        "    with open(csv_file, 'w') as csv:\n",
        "        csv.write('id,prediction\\n')\n",
        "        for tweet_id, pred in results:\n",
        "            csv.write(tweet_id)\n",
        "            csv.write(',')\n",
        "            csv.write(str(pred))\n",
        "            csv.write('\\n')\n",
        "\n",
        "\n",
        "def top_n_words(pkl_file_name, N, shift=0):\n",
        "    \"\"\"\n",
        "    Returns a dictionary of form {word:rank} of top N words from a pickle\n",
        "    file which has a nltk FreqDist object generated by stats.py\n",
        "    Args:\n",
        "        pkl_file_name (str): Name of pickle file\n",
        "        N (int): The number of words to get\n",
        "        shift: amount to shift the rank from 0.\n",
        "    Returns:\n",
        "        dict: Of form {word:rank}\n",
        "    \"\"\"\n",
        "    with open(pkl_file_name, 'rb') as pkl_file:\n",
        "        freq_dist = pickle.load(pkl_file)\n",
        "    most_common = freq_dist.most_common(N)\n",
        "    words = {p[0]: i + shift for i, p in enumerate(most_common)}\n",
        "    return words\n",
        "\n",
        "\n",
        "def top_n_bigrams(pkl_file_name, N, shift=0):\n",
        "    \"\"\"\n",
        "    Returns a dictionary of form {bigram:rank} of top N bigrams from a pickle\n",
        "    file which has a Counter object generated by stats.py\n",
        "    Args:\n",
        "        pkl_file_name (str): Name of pickle file\n",
        "        N (int): The number of bigrams to get\n",
        "        shift: amount to shift the rank from 0.\n",
        "    Returns:\n",
        "        dict: Of form {bigram:rank}\n",
        "    \"\"\"\n",
        "    with open(pkl_file_name, 'rb') as pkl_file:\n",
        "        freq_dist = pickle.load(pkl_file)\n",
        "    most_common = freq_dist.most_common(N)\n",
        "    bigrams = {p[0]: i for i, p in enumerate(most_common)}\n",
        "    return bigrams\n",
        "\n",
        "\n",
        "def split_data(tweets, validation_split=0.1):\n",
        "    \"\"\"Split the data into training and validation sets\n",
        "    Args:\n",
        "        tweets (list): list of tuples\n",
        "        validation_split (float, optional): validation split %\n",
        "    Returns:\n",
        "        (list, list): training-set, validation-set\n",
        "    \"\"\"\n",
        "    index = int((1 - validation_split) * len(tweets))\n",
        "    random.shuffle(tweets)\n",
        "    return tweets[:index], tweets[index:]"
      ],
      "metadata": {
        "id": "qV7WwOrtrNoJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHPyywx2f9-R",
        "outputId": "2e779036-8d9e-49c2-d5f5-f93ed5393688"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting utils\n",
            "  Downloading utils-1.0.1-py2.py3-none-any.whl (21 kB)\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 /content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/code/preprocess.py /content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/dataset/train.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_veP5sBYbm9D",
        "outputId": "28a669ff-0500-47c4-95ba-3519f331f3ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 100000/100000\n",
            "Saved processed tweets to: /content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 /content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/code/preprocess.py /content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/dataset/test.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JiBXGmZb656",
        "outputId": "818f40b1-4d8a-46e1-b5f9-d7aef8c77952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 300000/300000\n",
            "Saved processed tweets to: /content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/dataset/test-processed.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 /content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/code/stats.py /content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0oNcOLvccyi",
        "outputId": "e7f30d74-c621-43b3-e31b-73e7789faf84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 100000/100000\n",
            "Calculating frequency distribution\n",
            "Saved uni-frequency distribution to /content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist.pkl\n",
            "Saved bi-frequency distribution to /content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist-bi.pkl\n",
            "\n",
            "[Analysis Statistics]\n",
            "Tweets => Total: 100000, Positive: 56462, Negative: 43538\n",
            "User Mentions => Total: 88959, Avg: 0.8896, Max: 12\n",
            "URLs => Total: 3602, Avg: 0.0360, Max: 4\n",
            "Emojis => Total: 1001, Positive: 856, Negative: 145, Avg: 0.0100, Max: 5\n",
            "Words => Total: 1192424, Unique: 50376, Avg: 11.9242, Max: 40, Min: 0\n",
            "Bigrams => Total: 1092958, Unique: 385024, Avg: 10.9296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 /content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/code/stats.py /content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/dataset/test-processed.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bufwtub9cpJ1",
        "outputId": "2bae646f-980b-42e8-f62f-69ef34338a24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/code/stats.py\", line 63, in <module>\n",
            "    t_id, if_pos, tweet = line.strip().split(',')\n",
            "ValueError: need more than 2 values to unpack\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Baseline**"
      ],
      "metadata": {
        "id": "TLie8Nkvff-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 /content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/code/baseline.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKmMC89lfiHs",
        "outputId": "4d7daa0e-530b-430a-8173-d0600f9c02a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct = 65.35%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Naive Bayes**"
      ],
      "metadata": {
        "id": "_bn9iIAmgGpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 /content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/code/naivebayes.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sq_-rzmzgFso",
        "outputId": "d6b7365e-3d01-414f-d57f-f6bd24e1aa06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7815/10000 = 78.1500 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pNghMlUv6KoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Maximum Entropy**"
      ],
      "metadata": {
        "id": "Dz0-L5WRgy06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 /content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/code/maxent-nltk.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQ96oBdsg29b",
        "outputId": "efb341ce-98a3-4e31-e898-b6112c196c48"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ==> Training (1 iterations)\n",
            "\n",
            "      Iteration    Log Likelihood    Accuracy\n",
            "      ---------------------------------------\n",
            "             1          -0.69315        0.565\n",
            "         Final          -0.61624        0.756\n",
            "  -1.010 andyhurleyday==True and label is '0'\n",
            "   1.000 lancearmstrong==True and label is '0'\n",
            "   1.000 frenchiebday==True and label is '1'\n",
            "   1.000 ephesians==True and label is '1'\n",
            "   1.000 twitterart==True and label is '1'\n",
            "   1.000 madnessness==True and label is '0'\n",
            "   1.000 thestreetforce==True and label is '1'\n",
            "   1.000 atthepool==True and label is '1'\n",
            "   1.000 twilightnewmoon==True and label is '0'\n",
            "   1.000 hitech==True and label is '1'\n",
            "Validation set accuracy:0.0000\n",
            "\n",
            "Predicting for test data\n",
            "\n",
            "Saved to maxent.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Decision Tree**"
      ],
      "metadata": {
        "id": "hNwILHhx6tfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 /content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/code/decisiontree.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62tew1bF7MG4",
        "outputId": "a862e28e-e93e-4467-8d92-7eb1030bf5cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 6833/10000 = 68.3300 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Random Forest**"
      ],
      "metadata": {
        "id": "CuGITiUo7WPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 /content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/code/randomforest.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzDe5hqw7aug",
        "outputId": "1c2cb23e-fdb8-4c89-b0d7-3058de426ef6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1/usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7248/10000 = 72.4800 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **XGBoost**"
      ],
      "metadata": {
        "id": "sT-r2Typ7lib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from xgboost import XGBClassifier\n",
        "from scipy.sparse import lil_matrix\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# Performs classification using XGBoost.\n",
        "\n",
        "\n",
        "FREQ_DIST_FILE = '/content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/dataset/test-processed.csv'\n",
        "TRAIN = True\n",
        "UNIGRAM_SIZE = 15000\n",
        "VOCAB_SIZE = UNIGRAM_SIZE\n",
        "USE_BIGRAMS = True\n",
        "if USE_BIGRAMS:\n",
        "    BIGRAM_SIZE = 10000\n",
        "    VOCAB_SIZE = UNIGRAM_SIZE + BIGRAM_SIZE\n",
        "FEAT_TYPE = 'frequency'\n",
        "\n",
        "\n",
        "def get_feature_vector(tweet):\n",
        "    uni_feature_vector = []\n",
        "    bi_feature_vector = []\n",
        "    words = tweet.split()\n",
        "    for i in xrange(len(words) - 1):\n",
        "        word = words[i]\n",
        "        next_word = words[i + 1]\n",
        "        if unigrams.get(word):\n",
        "            uni_feature_vector.append(word)\n",
        "        if USE_BIGRAMS:\n",
        "            if bigrams.get((word, next_word)):\n",
        "                bi_feature_vector.append((word, next_word))\n",
        "    if len(words) >= 1:\n",
        "        if unigrams.get(words[-1]):\n",
        "            uni_feature_vector.append(words[-1])\n",
        "    return uni_feature_vector, bi_feature_vector\n",
        "\n",
        "\n",
        "def extract_features(tweets, batch_size=500, test_file=True, feat_type='presence'):\n",
        "    num_batches = int(np.ceil(len(tweets) / float(batch_size)))\n",
        "    for i in xrange(num_batches):\n",
        "        batch = tweets[i * batch_size: (i + 1) * batch_size]\n",
        "        features = lil_matrix((batch_size, VOCAB_SIZE))\n",
        "        labels = np.zeros(batch_size)\n",
        "        for j, tweet in enumerate(batch):\n",
        "            if test_file:\n",
        "                tweet_words = tweet[1][0]\n",
        "                tweet_bigrams = tweet[1][1]\n",
        "            else:\n",
        "                tweet_words = tweet[2][0]\n",
        "                tweet_bigrams = tweet[2][1]\n",
        "                labels[j] = tweet[1]\n",
        "            if feat_type == 'presence':\n",
        "                tweet_words = set(tweet_words)\n",
        "                tweet_bigrams = set(tweet_bigrams)\n",
        "            for word in tweet_words:\n",
        "                idx = unigrams.get(word)\n",
        "                if idx:\n",
        "                    features[j, idx] += 1\n",
        "            if USE_BIGRAMS:\n",
        "                for bigram in tweet_bigrams:\n",
        "                    idx = bigrams.get(bigram)\n",
        "                    if idx:\n",
        "                        features[j, UNIGRAM_SIZE + idx] += 1\n",
        "        yield features, labels\n",
        "\n",
        "\n",
        "def apply_tf_idf(X):\n",
        "    transformer = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)\n",
        "    transformer.fit(X)\n",
        "    return transformer\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    \"\"\"Returns a list of tuples of type (tweet_id, feature_vector)\n",
        "            or (tweet_id, sentiment, feature_vector)\n",
        "    Args:\n",
        "        csv_file (str): Name of processed csv file generated by preprocess.py\n",
        "        test_file (bool, optional): If processing test file\n",
        "    Returns:\n",
        "        list: Of tuples\n",
        "    \"\"\"\n",
        "    tweets = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append((tweet_id, feature_vector))\n",
        "            else:\n",
        "                tweets.append((tweet_id, int(sentiment), feature_vector))\n",
        "            write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    np.random.seed(1337)\n",
        "    unigrams = top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
        "    if USE_BIGRAMS:\n",
        "        bigrams = top_n_bigrams(BI_FREQ_DIST_FILE, BIGRAM_SIZE)\n",
        "    tweets = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    if TRAIN:\n",
        "        train_tweets, val_tweets = split_data(tweets)\n",
        "    else:\n",
        "        random.shuffle(tweets)\n",
        "        train_tweets = tweets\n",
        "    del tweets\n",
        "    print ('Extracting features & training batches')\n",
        "    clf = XGBClassifier(max_depth=25, silent=False, n_estimators=400)\n",
        "    batch_size = len(train_tweets)\n",
        "    i = 1\n",
        "    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))\n",
        "    for training_set_X, training_set_y in extract_features(train_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "        write_status(i, n_train_batches)\n",
        "        i += 1\n",
        "        if FEAT_TYPE == 'frequency':\n",
        "            tfidf = apply_tf_idf(training_set_X)\n",
        "            training_set_X = tfidf.transform(training_set_X)\n",
        "        clf.fit(training_set_X, training_set_y)\n",
        "    print ('\\n')\n",
        "    print ('Testing')\n",
        "    if TRAIN:\n",
        "        correct, total = 0, len(val_tweets)\n",
        "        i = 1\n",
        "        batch_size = len(val_tweets)\n",
        "        n_val_batches = int(np.ceil(len(val_tweets) / float(batch_size)))\n",
        "        for val_set_X, val_set_y in extract_features(val_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                val_set_X = tfidf.transform(val_set_X)\n",
        "            prediction = clf.predict(val_set_X)\n",
        "            correct += np.sum(prediction == val_set_y)\n",
        "            write_status(i, n_val_batches)\n",
        "            i += 1\n",
        "        print ('\\nCorrect: %d/%d = %.4f %%' % (correct, total, correct * 100. / total))\n",
        "    else:\n",
        "        del train_tweets\n",
        "        test_tweets = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "        n_test_batches = int(np.ceil(len(test_tweets) / float(batch_size)))\n",
        "        predictions = np.array([])\n",
        "        print ('Predicting batches')\n",
        "        i = 1\n",
        "        for test_set_X, _ in extract_features(test_tweets, test_file=True, feat_type=FEAT_TYPE):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                test_set_X = tfidf.transform(test_set_X)\n",
        "            prediction = clf.predict(test_set_X)\n",
        "            predictions = np.concatenate((predictions, prediction))\n",
        "            write_status(i, n_test_batches)\n",
        "            i += 1\n",
        "        predictions = [(str(j), int(predictions[j]))\n",
        "                       for j in range(len(test_tweets))]\n",
        "        save_results_to_csv(predictions, 'xgboost.csv')\n",
        "        print ('\\nSaved to xgboost.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfTu44qW7qVc",
        "outputId": "1fffa1f4-a069-4794-80fc-802d9dc2200a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7677/10000 = 76.7700 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SVM**"
      ],
      "metadata": {
        "id": "rolqsZf2748l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 /content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/code/svm.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJ2-NrHR77tA",
        "outputId": "1ae93740-5dce-48c8-d35c-7ff888390da7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7856/10000 = 78.5600 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Multi-Layer Perceptron**"
      ],
      "metadata": {
        "id": "e7dDMbr68FsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Performs classification using an MLP/1-hidden-layer NN.\n",
        "\n",
        "FREQ_DIST_FILE = '/content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/dataset/test-processed.csv'\n",
        "xrange=range\n",
        "TRAIN = True\n",
        "UNIGRAM_SIZE = 15000\n",
        "VOCAB_SIZE = UNIGRAM_SIZE\n",
        "USE_BIGRAMS = False\n",
        "if USE_BIGRAMS:\n",
        "    BIGRAM_SIZE = 10000\n",
        "    VOCAB_SIZE = UNIGRAM_SIZE + BIGRAM_SIZE\n",
        "FEAT_TYPE = 'frequency'\n",
        "\n",
        "\n",
        "def get_feature_vector(tweet):\n",
        "    uni_feature_vector = []\n",
        "    bi_feature_vector = []\n",
        "    words = tweet.split()\n",
        "    for i in xrange(len(words) - 1):\n",
        "        word = words[i]\n",
        "        next_word = words[i + 1]\n",
        "        if unigrams.get(word):\n",
        "            uni_feature_vector.append(word)\n",
        "        if USE_BIGRAMS:\n",
        "            if bigrams.get((word, next_word)):\n",
        "                bi_feature_vector.append((word, next_word))\n",
        "    if len(words) >= 1:\n",
        "        if unigrams.get(words[-1]):\n",
        "            uni_feature_vector.append(words[-1])\n",
        "    return uni_feature_vector, bi_feature_vector\n",
        "\n",
        "\n",
        "def extract_features(tweets, batch_size=500, test_file=True, feat_type='presence'):\n",
        "    num_batches = int(np.ceil(len(tweets) / float(batch_size)))\n",
        "    for i in xrange(num_batches):\n",
        "        batch = tweets[i * batch_size: (i + 1) * batch_size]\n",
        "        features = np.zeros((batch_size, VOCAB_SIZE))\n",
        "        labels = np.zeros(batch_size)\n",
        "        for j, tweet in enumerate(batch):\n",
        "            if test_file:\n",
        "                tweet_words = tweet[1][0]\n",
        "                tweet_bigrams = tweet[1][1]\n",
        "            else:\n",
        "                tweet_words = tweet[2][0]\n",
        "                tweet_bigrams = tweet[2][1]\n",
        "                labels[j] = tweet[1]\n",
        "            if feat_type == 'presence':\n",
        "                tweet_words = set(tweet_words)\n",
        "                tweet_bigrams = set(tweet_bigrams)\n",
        "            for word in tweet_words:\n",
        "                idx = unigrams.get(word)\n",
        "                if idx:\n",
        "                    features[j, idx] += 1\n",
        "            if USE_BIGRAMS:\n",
        "                for bigram in tweet_bigrams:\n",
        "                    idx = bigrams.get(bigram)\n",
        "                    if idx:\n",
        "                        features[j, UNIGRAM_SIZE + idx] += 1\n",
        "        yield features, labels\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    tweets = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append((tweet_id, feature_vector))\n",
        "            else:\n",
        "                tweets.append((tweet_id, int(sentiment), feature_vector))\n",
        "            write_status(i + 1, total)\n",
        "    print('\\n')\n",
        "    return tweets\n",
        "\n",
        "\n",
        "def build_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(500, input_dim=VOCAB_SIZE, activation='sigmoid'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def evaluate_model(model, val_tweets):\n",
        "    correct, total = 0, len(val_tweets)\n",
        "    for val_set_X, val_set_y in extract_features(val_tweets, feat_type=FEAT_TYPE, test_file=False):\n",
        "        prediction = model.predict_on_batch(val_set_X)\n",
        "        prediction = np.round(prediction)\n",
        "        correct += np.sum(prediction == val_set_y[:, None])\n",
        "    return float(correct) / total\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    np.random.seed(1337)\n",
        "    unigrams = top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
        "    if USE_BIGRAMS:\n",
        "        bigrams = top_n_bigrams(BI_FREQ_DIST_FILE, BIGRAM_SIZE)\n",
        "    tweets = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    if TRAIN:\n",
        "        train_tweets, val_tweets = split_data(tweets)\n",
        "    else:\n",
        "        random.shuffle(tweets)\n",
        "        train_tweets = tweets\n",
        "    del tweets\n",
        "    print ('Extracting features & training batches')\n",
        "    nb_epochs = 5\n",
        "    batch_size = 500\n",
        "    model = build_model()\n",
        "    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))\n",
        "    best_val_acc = 0.0\n",
        "    for j in xrange(nb_epochs):\n",
        "        i = 1\n",
        "        for training_set_X, training_set_y in extract_features(train_tweets, feat_type=FEAT_TYPE, batch_size=batch_size, test_file=False):\n",
        "            o = model.train_on_batch(training_set_X, training_set_y)\n",
        "            sys.stdout.write('\\rIteration %d/%d, loss:%.4f, acc:%.4f' %\n",
        "                             (i, n_train_batches, o[0], o[1]))\n",
        "            sys.stdout.flush()\n",
        "            i += 1\n",
        "        val_acc = evaluate_model(model, val_tweets)\n",
        "        print ('\\nEpoch: %d, val_acc:%.4f' % (j + 1, val_acc))\n",
        "        random.shuffle(train_tweets)\n",
        "        if val_acc > best_val_acc:\n",
        "            print ('Accuracy improved from %.4f to %.4f, saving model' % (best_val_acc, val_acc))\n",
        "            best_val_acc = val_acc\n",
        "            model.save('best_model.h5')\n",
        "    print ('Testing')\n",
        "    del train_tweets\n",
        "    del model\n",
        "    model = load_model('best_model.h5')\n",
        "    test_tweets = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "    n_test_batches = int(np.ceil(len(test_tweets) / float(batch_size)))\n",
        "    predictions = np.array([])\n",
        "    print ('Predicting batches')\n",
        "    i = 1\n",
        "    for test_set_X, _ in extract_features(test_tweets, feat_type=FEAT_TYPE, batch_size=batch_size, test_file=True):\n",
        "        prediction = np.round(model.predict_on_batch(test_set_X).flatten())\n",
        "        predictions = np.concatenate((predictions, prediction))\n",
        "        write_status(i, n_test_batches)\n",
        "        i += 1\n",
        "    predictions = [(str(j), int(predictions[j]))\n",
        "                   for j in range(len(test_tweets))]\n",
        "    save_results_to_csv(predictions, '1layerneuralnet.csv')\n",
        "    print ('\\nSaved to 1layerneuralnet.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvbS0ZVz8KV5",
        "outputId": "381507ec-b5ab-4893-ab7e-d7851e4d1ce3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Iteration 180/180, loss:0.4970, acc:0.7800\n",
            "Epoch: 1, val_acc:0.7544\n",
            "Accuracy improved from 0.0000 to 0.7544, saving model\n",
            "Iteration 180/180, loss:0.4749, acc:0.7980\n",
            "Epoch: 2, val_acc:0.7651\n",
            "Accuracy improved from 0.7544 to 0.7651, saving model\n",
            "Iteration 180/180, loss:0.4254, acc:0.8360\n",
            "Epoch: 3, val_acc:0.7643\n",
            "Iteration 180/180, loss:0.4722, acc:0.7720\n",
            "Epoch: 4, val_acc:0.7601\n",
            "Iteration 180/180, loss:0.4134, acc:0.8160\n",
            "Epoch: 5, val_acc:0.7586\n",
            "Testing\n",
            "Generating feature vectors\n",
            "Processing 300000/300000\n",
            "\n",
            "Predicting batches\n",
            "Processing 600/600\n",
            "Saved to 1layerneuralnet.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Reccurent Neural Network**"
      ],
      "metadata": {
        "id": "ppLZjOLn8XdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Embedding\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.layers import LSTM\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Performs classification using LSTM network.\n",
        "\n",
        "FREQ_DIST_FILE = '/content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/dataset/test-processed.csv'\n",
        "GLOVE_FILE = '/content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/dataset/glove-seeds.txt'\n",
        "dim = 200\n",
        "\n",
        "\n",
        "def get_glove_vectors(vocab):\n",
        "    print ('Looking for GLOVE vectors')\n",
        "    glove_vectors = {}\n",
        "    found = 0\n",
        "    with open(GLOVE_FILE, 'r') as glove_file:\n",
        "        for i, line in enumerate(glove_file):\n",
        "            write_status(i + 1, 0)\n",
        "            tokens = line.split()\n",
        "            word = tokens[0]\n",
        "            if vocab.get(word):\n",
        "                vector = [float(e) for e in tokens[1:]]\n",
        "                glove_vectors[word] = np.array(200)\n",
        "                found += 1\n",
        "    print ('\\n')\n",
        "    print ('Found %d words in GLOVE' % found)\n",
        "    return glove_vectors\n",
        "\n",
        "\n",
        "def get_feature_vector(tweet):\n",
        "    words = tweet.split()\n",
        "    feature_vector = []\n",
        "    for i in range(len(words) - 1):\n",
        "        word = words[i]\n",
        "        if vocab.get(word) is not None:\n",
        "            feature_vector.append(vocab.get(word))\n",
        "    if len(words) >= 1:\n",
        "        if vocab.get(words[-1]) is not None:\n",
        "            feature_vector.append(vocab.get(words[-1]))\n",
        "    return feature_vector\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    tweets = []\n",
        "    labels = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append(feature_vector)\n",
        "            else:\n",
        "                tweets.append(feature_vector)\n",
        "                labels.append(int(sentiment))\n",
        "            write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets, np.array(labels)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train = len(sys.argv) == 1\n",
        "    np.random.seed(1337)\n",
        "    vocab_size = 90000\n",
        "    batch_size = 500\n",
        "    max_length = 40\n",
        "    filters = 600\n",
        "    kernel_size = 3\n",
        "    vocab = top_n_words(FREQ_DIST_FILE, vocab_size, shift=1)\n",
        "    glove_vectors = get_glove_vectors(vocab)\n",
        "    tweets, labels = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    embedding_matrix = np.random.randn(vocab_size + 1, dim) * 0.01\n",
        "    for word, i in vocab.items():\n",
        "        glove_vector = glove_vectors.get(word)\n",
        "        if glove_vector is not None:\n",
        "            embedding_matrix[i] = glove_vector\n",
        "    tweets = pad_sequences(tweets, maxlen=max_length, padding='post')\n",
        "    shuffled_indices = np.random.permutation(tweets.shape[0])\n",
        "    tweets = tweets[shuffled_indices]\n",
        "    labels = labels[shuffled_indices]\n",
        "    if train:\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(vocab_size + 1, dim, weights=[embedding_matrix], input_length=max_length))\n",
        "        model.add(Dropout(0.4))\n",
        "        model.add(LSTM(128))\n",
        "        model.add(Dense(64))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(Dense(1))\n",
        "        model.add(Activation('sigmoid'))\n",
        "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        filepath = \"./models/lstm-{epoch:02d}-{loss:0.3f}-{acc:0.3f}-{val_loss:0.3f}-{val_acc:0.3f}.hdf5\"\n",
        "        checkpoint = ModelCheckpoint(filepath, monitor=\"loss\", verbose=1, save_best_only=True, mode='min')\n",
        "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.000001)\n",
        "        print ('model.summary()')\n",
        "        model.fit(tweets, labels, batch_size=128, epochs=5, validation_split=0.1, shuffle=True, callbacks=[checkpoint, reduce_lr])\n",
        "    else:\n",
        "        model = load_model(sys.argv[1])\n",
        "        print ('model.summary()')\n",
        "        test_tweets, _ = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "        test_tweets = pad_sequences(test_tweets, maxlen=max_length, padding='post')\n",
        "        predictions = model.predict(test_tweets, batch_size=128, verbose=1)\n",
        "        results = zip(map(str, range(len(test_tweets))), np.round(predictions[:, 0]).astype(int))\n",
        "        save_results_to_csv(results, 'lstm.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "SvllLoNw8brx",
        "outputId": "4a7e06d3-9656-4e3a-d055-c4fae3f6ae06"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for GLOVE vectors\n",
            "Processing 15213/0"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-11bffdff5604>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_n_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFREQ_DIST_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mglove_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_glove_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mtweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_PROCESSED_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-11bffdff5604>\u001b[0m in \u001b[0;36mget_glove_vectors\u001b[0;34m(vocab)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGLOVE_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mglove_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mwrite_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-80c5c2cffa44>\u001b[0m in \u001b[0;36mwrite_status\u001b[0;34m(i, total)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Processing %d/%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0;31m# and give a timeout to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m                     \u001b[0;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                     \u001b[0;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Convolution Neural Network**"
      ],
      "metadata": {
        "id": "HLIoDXGm8qxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Embedding, Flatten\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Performs classification using CNN.\n",
        "\n",
        "FREQ_DIST_FILE = '/content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/dataset/test-processed.csv'\n",
        "GLOVE_FILE = '/content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/dataset/glove-seeds.txt'\n",
        "dim = 200\n",
        "\n",
        "\n",
        "def get_glove_vectors(vocab):\n",
        "    \"\"\"\n",
        "    Extracts glove vectors from seed file only for words present in vocab.\n",
        "    \"\"\"\n",
        "    print ('Looking for GLOVE seeds')\n",
        "    glove_vectors = {}\n",
        "    found = 0\n",
        "    with open(GLOVE_FILE, 'r') as glove_file:\n",
        "        for i, line in enumerate(glove_file):\n",
        "            write_status(i + 1, 0)\n",
        "            tokens = line.strip().split()\n",
        "            word = tokens[0]\n",
        "            if vocab.get(word):\n",
        "                vector = [float(e) for e in tokens[1:]]\n",
        "                glove_vectors[word] = np.array(vector)\n",
        "                found += 1\n",
        "    print ('\\n')\n",
        "    return glove_vectors\n",
        "\n",
        "\n",
        "def get_feature_vector(tweet):\n",
        "    \"\"\"\n",
        "    Generates a feature vector for each tweet where each word is\n",
        "    represented by integer index based on rank in vocabulary.\n",
        "    \"\"\"\n",
        "    words = tweet.split()\n",
        "    feature_vector = []\n",
        "    for i in range(len(words) - 1):\n",
        "        word = words[i]\n",
        "        if vocab.get(word) is not None:\n",
        "            feature_vector.append(vocab.get(word))\n",
        "    if len(words) >= 1:\n",
        "        if vocab.get(words[-1]) is not None:\n",
        "            feature_vector.append(vocab.get(words[-1]))\n",
        "    return feature_vector\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    \"\"\"\n",
        "    Generates training X, y pairs.\n",
        "    \"\"\"\n",
        "    tweets = []\n",
        "    labels = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append(feature_vector)\n",
        "            else:\n",
        "                tweets.append(feature_vector)\n",
        "                labels.append(int(sentiment))\n",
        "            write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets, np.array(labels)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train = len(sys.argv) == 1\n",
        "    np.random.seed(1337)\n",
        "    vocab_size = 90000\n",
        "    batch_size = 500\n",
        "    max_length = 40\n",
        "    filters = 600\n",
        "    kernel_size = 3\n",
        "    vocab = top_n_words(FREQ_DIST_FILE, vocab_size, shift=1)\n",
        "    glove_vectors = get_glove_vectors(vocab)\n",
        "    tweets, labels = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    # Create and embedding matrix\n",
        "    embedding_matrix = np.random.randn(vocab_size + 1, dim) * 0.01\n",
        "    # Seed it with GloVe vectors\n",
        "    for word, i in vocab.items():\n",
        "        glove_vector = glove_vectors.get(word)\n",
        "        if glove_vector is not None:\n",
        "            embedding_matrix[i] = glove_vector\n",
        "    tweets = pad_sequences(tweets, maxlen=max_length, padding='post')\n",
        "    shuffled_indices = np.random.permutation(tweets.shape[0])\n",
        "    tweets = tweets[shuffled_indices]\n",
        "    labels = labels[shuffled_indices]\n",
        "    if train:\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(vocab_size + 1, dim, weights=[embedding_matrix], input_length=max_length))\n",
        "        model.add(Dropout(0.4))\n",
        "        model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1))\n",
        "        model.add(Conv1D(300, kernel_size, padding='valid', activation='relu', strides=1))\n",
        "        model.add(Conv1D(150, kernel_size, padding='valid', activation='relu', strides=1))\n",
        "        model.add(Conv1D(75, kernel_size, padding='valid', activation='relu', strides=1))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(600))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(Dense(1))\n",
        "        model.add(Activation('sigmoid'))\n",
        "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        filepath = \"./models/4cnn-{epoch:02d}-{loss:0.3f}-{acc:0.3f}-{val_loss:0.3f}-{val_acc:0.3f}.hdf5\"\n",
        "        checkpoint = ModelCheckpoint(filepath, monitor=\"loss\", verbose=1, save_best_only=True, mode='min')\n",
        "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.000001)\n",
        "        model.fit(tweets, labels, batch_size=128, epochs=8, validation_split=0.1, shuffle=True, callbacks=[checkpoint, reduce_lr])\n",
        "    else:\n",
        "        model = load_model(sys.argv[1])\n",
        "        print model.summary()\n",
        "        test_tweets, _ = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "        test_tweets = pad_sequences(test_tweets, maxlen=max_length, padding='post')\n",
        "        predictions = model.predict(test_tweets, batch_size=128, verbose=1)\n",
        "        results = zip(map(str, range(len(test_tweets))), np.round(predictions[:, 0]).astype(int))\n",
        "        save_results_to_csv(results, 'cnn.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcKIW0iK8z1S",
        "outputId": "a6a10b9d-e228-4d33-ba20-2dc28db19876"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/code/cnn.py\", line 3, in <module>\n",
            "    from keras.models import Sequential, load_model\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/keras/__init__.py\", line 21, in <module>\n",
            "    from tensorflow.python import tf2\n",
            "ImportError: No module named tensorflow.python\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Majority Vote Ensemble**"
      ],
      "metadata": {
        "id": "o_LqhDlh9PJy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Extract-CNN-Feast**"
      ],
      "metadata": {
        "id": "pVi57_j0DrvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "from keras.models import load_model, Model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Extracts dense vector features from penultimate layer of CNN model.\n",
        "\n",
        "FREQ_DIST_FILE = '/content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/dataset/test-processed.csv'\n",
        "GLOVE_FILE = '/content/drive/MyDrive/Machine_Learning/Twitter-Sentiment-Analysis-main/dataset/glove-seeds.txt'\n",
        "dim = 200\n",
        "\n",
        "\n",
        "def get_glove_vectors(vocab):\n",
        "    print ('Looking for GLOVE seeds')\n",
        "    glove_vectors = {}\n",
        "    found = 0\n",
        "    with open(GLOVE_FILE, 'r') as glove_file:\n",
        "        for i, line in enumerate(glove_file):\n",
        "            write_status(i + 1, 0)\n",
        "            tokens = line.strip().split()\n",
        "            word = tokens[0]\n",
        "            if vocab.get(word):\n",
        "                vector = [float(e) for e in tokens[1:]]\n",
        "                glove_vectors[word] = np.array(vector)\n",
        "                found += 1\n",
        "    print ('\\n')\n",
        "    return glove_vectors\n",
        "\n",
        "\n",
        "def get_feature_vector(tweet):\n",
        "    words = tweet.split()\n",
        "    feature_vector = []\n",
        "    for i in range(len(words) - 1):\n",
        "        word = words[i]\n",
        "        if vocab.get(word) is not None:\n",
        "            feature_vector.append(vocab.get(word))\n",
        "    if len(words) >= 1:\n",
        "        if vocab.get(words[-1]) is not None:\n",
        "            feature_vector.append(vocab.get(words[-1]))\n",
        "    return feature_vector\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    tweets = []\n",
        "    labels = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append(feature_vector)\n",
        "            else:\n",
        "                tweets.append(feature_vector)\n",
        "                labels.append(int(sentiment))\n",
        "            write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets, np.array(labels)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train = len(sys.argv) == 1\n",
        "    np.random.seed(1337)\n",
        "    vocab_size = 90000\n",
        "    batch_size = 500\n",
        "    max_length = 40\n",
        "    filters = 600\n",
        "    kernel_size = 3\n",
        "    vocab = top_n_words(FREQ_DIST_FILE, vocab_size, shift=1)\n",
        "    glove_vectors = get_glove_vectors(vocab)\n",
        "    tweets, labels = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    tweets = pad_sequences(tweets, maxlen=max_length, padding='post')\n",
        "    shuffled_indices = np.random.permutation(tweets.shape[0])\n",
        "    tweets = tweets[shuffled_indices]\n",
        "    labels = labels[shuffled_indices]\n",
        "    model = load_model(sys.argv[1])\n",
        "    model = Model(model.layers[0].input, model.layers[-3].output)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    print model.summary()\n",
        "    test_tweets, _ = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "    test_tweets = pad_sequences(test_tweets, maxlen=max_length, padding='post')\n",
        "    predictions = model.predict(test_tweets, batch_size=1024, verbose=1)\n",
        "    np.save('test-feats.npy', predictions)\n",
        "    predictions = model.predict(tweets, batch_size=1024, verbose=1)\n",
        "    np.save('train-feats.npy', predictions)\n",
        "    np.savetxt('train-labels.txt', labels)\n"
      ],
      "metadata": {
        "id": "P2pzuCTzC_EC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CNN-Feast-SVM**"
      ],
      "metadata": {
        "id": "iT1yI97YDxYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZERPV6SGD3kR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}